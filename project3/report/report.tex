\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Introduction to Learning and Intelligent Systems - Spring 2015}
\author{jmohan@student.ethz.ch\\ nleow@student.ethz.ch\\ wongs@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Project 3 : Image Classification}

As this is a classification problem with 10 labels and 2048 features, we used the following methods

\begin{itemize}
  \item Random Forest
  \item Extra Random Trees
  \item Decision Tree Classifier
  \item Ada Boost
  \item Linear Discriminant Analysis
  \item Gradient Boosting
  \item Naive Bayes
  \item Nearest Centroid
  \item Nearest Neighbours
  \item Deep Belief Networks
\end{itemize}

In general, we deduced the following
\begin{itemize}
  \item Trees, Gradient Boosting resulted in long training times
  \item Normalization of data did not improve the prediction metric
  \item Trees and naive bayes on various heuristics could not break the easy benchmark
  \item Linear Discriminant Analysis with Single value decomposition met the easy benchmark.
  \item Deep Belief Networks gave the best prediction metrics
\end{itemize}

CUDAMat was used on Deep Belief Networks to shorten training times by at least 10x. This also resulted in a large improvement of scores.

We do not yet understand why increasing the number of layers in the network increases performance. Stack overflow sources recommended a range between the number of features and number of classifications.

A modal method was used to boost the results of the deep belief networks.

\end{document}
